1688-java-研发
淘天集团 · 杭州

收录时间： 2025年03月08日

职位描述
1. 负责阿里B类商业化产品研发，帮助商家在1688做好生意，能主导商业化产品核心架构设计及研发工作；
2. 通过对业务的理解发现系统瓶颈，对现有系统进行全面升级，提升系统性能和扩展性；
3. 负责团队技术；
大模型应用；
预研和技术难点攻关，保障系统可用性、稳定性、和可扩展性；
职位要求
1、本科及以上学历，计算机相关专业；
1年以上相关岗位工作经验，精通java编程，有大型互联网公司工作经验者优先；
2、熟悉分布式系统的设计和应用，熟悉分布式、缓存、消息等机制；
能对分布式常用技术进行合理应用，解决问题；
3、掌握多线程及高性能的设计与编码及性能调优；
有高并发应用开发经验者优先；
4、掌握Linux操作系统和大型数据库；
Oracle、MySql；
熟悉网络通信，有较好的计算机基础和较强的分析、处理能力；
5、具备良好的抽象设计能力，思路清晰，善于思考，能独立分析和解决问题，责任心强，具备良好的团队合作精神和承受压力的能力；



问题: 请描述你在大型互联网公司中主导商业化产品核心架构设计及研发工作的具体经历，并说明当时是如何解决架构设计中遇到的关键问题的
答案: 在某头部互联网公司负责广告商业化平台核心架构设计时，我主导了从0到1的架构升级，支撑日均10亿+曝光、千万级QPS的广告投放业务。当时业务核心诉求是兼顾高并发、实时计费准确性与多租户资源隔离，我带领15人技术团队完成架构设计与研发落地。

架构设计阶段，先梳理业务域：将投放管理、实时计费、效果归因拆分为独立微服务，通过Kafka实现异步通信解耦；数据层采用分库分表（按用户ID哈希）+ Redis集群缓存热点广告，保障低延迟；引入Flink实时计算处理曝光数据，支撑实时报表。

核心问题1：高并发下计费系统数据一致性。初期压测发现，峰值QPS达300万时，数据库写入延迟超2s，偶现计费金额偏差。根因分析后，我们设计“预扣+异步结算”机制：请求时Redis预扣额度（设10s过期），异步线程批量落库，结合TCC事务补偿未成功订单；同时用ZooKeeper分布式锁防止重复计费，最终实现计费准确率99.99%，延迟降至50ms内。

核心问题2：多租户资源抢占。大客户与中小客户共用资源池时，曾因大客户突发放量导致小客户接口超时率升至15%。我们设计租户资源隔离层：按客户等级划分铂金/普通池，通过K8s HPA实现资源弹性调度，普通池超阈值时自动扩容，铂金池预留20%冗余资源。上线后小客户超时率降至0.3%，资源利用率提升40%。

研发中通过3轮架构评审（联合业务、运维团队）确定方案，采用灰度发布（先切10%流量）验证，最终支撑了业务3倍增长，年营收贡献超20亿。
--------------------------------------------------
问题: 当你发现现有系统存在瓶颈时，如何进行系统全面升级以提升性能和扩展性？请举例说明你运用的具体技术和方法
答案: 发现系统瓶颈时，我会先通过全链路监控（如Prometheus+Grafana）和日志分析（ELK）定位核心问题，结合压测工具（JMeter）模拟流量，确定瓶颈类型（如资源瓶颈、架构瓶颈或代码瓶颈）。升级时遵循“先优化后扩展”原则，从应用、数据、架构层分层突破。

例如之前电商项目中，促销活动时订单系统TPS仅300，响应超时率超15%。通过监控发现瓶颈在数据库：订单表单表数据超5000万条，写入并发达2000+/秒，索引失效导致查询慢。  
解决时，先从应用层优化：用Redis缓存商品库存、用户地址等热点数据，查询命中率提升至80%；非核心流程（如短信通知）通过RabbitMQ异步化，减少主流程阻塞。数据层：采用Sharding-JDBC按用户ID哈希分库分表，订单表拆为16库64表，单表数据量降至80万条；优化索引（新增订单状态+创建时间复合索引），慢查询减少90%。架构层：部署Nginx负载均衡，水平扩展3台应用服务器，数据库读写分离（主库写入、从库查询）。  
最终系统TPS提升至1200+，响应时间从600ms降至120ms，支撑了单日10万订单量，零超时。
--------------------------------------------------
问题: 在团队技术大模型应用方面，你有哪些尝试或经验？请阐述你是如何将大模型应用到具体研发工作中的
答案: 在团队研发中，我主要从代码提效、文档管理、测试优化三个方向推动大模型落地。代码层面，针对团队API调用频繁的问题，训练了基于内部接口文档的定制化代码生成模型，新功能开发时自动生成80%基础调用代码，人工聚焦逻辑优化，使中等复杂度模块开发周期缩短30%，低级语法错误减少25%。文档管理上，用大模型处理历史技术文档，自动提取核心逻辑生成接口手册，并实时同步代码更新日志，文档整理人力成本降低80%，版本一致性从75%提升至92%。测试环节，通过大模型解析需求文档生成测试用例，重点补充边缘场景（如异常输入、高并发边界），测试用例产出效率提升40%，覆盖度从82%升至95%，漏测率下降15%。此外搭建内部知识库问答机器人，整合手册与历史问题，新成员上手业务平均周期缩短50%。整体来看，大模型在研发全流程中有效减少重复劳动，支撑团队迭代效率提升约25%。
--------------------------------------------------
问题: 请分享你在预研和技术难点攻关过程中的一个案例，包括遇到的技术难点、采取的攻关策略以及最终的成果
答案: 在之前负责的供应链数据中台项目中，我曾主导解决高并发场景下的实时数据同步难题。当时业务要求将上游200+分表的订单数据实时同步至下游分析系统，延迟需控制在500ms内，且数据准确率达99.99%。但初期使用的Flink CDC工具在分表场景下出现严重问题：因分表路由规则动态变化，常出现变更事件重复抓取（重复率约15%），且同步延迟高达3-5秒，无法满足需求。

技术难点主要有三：一是分表路由规则由业务系统动态生成，CDC工具无法提前订阅全量表；二是分表间数据存在交叉引用，单表同步易导致关联数据不一致；三是峰值时段每秒10万+变更事件，并发处理易引发数据错乱。

攻关时，我先通过日志分析和压力测试定位根因：分表元数据更新不及时导致CDC订阅遗漏，且分表间无分布式事务控制。策略上，我分三步解决：1. 预研阶段对比Debezium、Canal等工具，发现均无法适配动态分表场景，遂决定自研核心模块；2. 设计“动态分表订阅引擎”：基于业务路由规则开发元数据监听服务，实时捕获分表新增/删除事件，自动生成CDC订阅任务；3. 引入“事件溯源”机制：将分表变更事件统一写入Kafka，通过业务ID聚合关联数据，再用Redis分布式锁控制单ID并发处理，最后以Flink状态后端缓存中间结果，确保数据一致性。

实施后，数据同步延迟从3秒降至200ms内，重复率从15%压降至0.05%，准确率达标。该方案支撑了下游库存预警系统的实时决策，使补货响应速度提升40%，并被纳入公司技术中台标准方案，后续推广至3个业务线复用。
--------------------------------------------------
问题: 谈谈你对分布式系统设计和应用的理解，以及在实际工作中如何合理应用分布式常用技术来解决问题
答案: 分布式系统是通过多节点网络协同完成任务的架构，核心是解决单机资源、性能和容错瓶颈，设计需平衡高可用、可扩展性、一致性等目标。其关键挑战包括网络不可靠性（延迟/分区）、数据一致性（多副本同步）和节点容错，需基于CAP理论权衡——多数场景优先保证分区容错性，再根据业务选可用性（如社交Feed）或一致性（如金融交易）；BASE理论则通过“基本可用、软状态、最终一致”降低强一致成本，是分布式实践的核心指导。

实际工作中，我会结合业务场景落地技术。比如做电商订单系统时，面对峰值流量和海量数据，分三方面设计：一是流量治理，用Nginx+K8s负载均衡分散请求，热点商品页通过CDN和Redis集群缓存（主从+哨兵保证可用性）；二是数据存储，订单表按用户ID哈希分库分表（解决单表性能瓶颈），历史订单归档至MongoDB分片集群；三是服务可靠性，微服务用Dubbo调用，Eureka做注册发现，集成Sentinel熔断降级（当库存服务异常时降级为本地缓存兜底）；支付环节采用TCC模式保证事务一致性（Try扣减库存、Confirm提交订单、Cancel回滚），异步通知通过Kafka解耦（重试机制确保最终送达）。

设计的关键是“取舍”：非核心链路（如物流通知）用Kafka异步+最终一致性，优先保障主链路（下单-支付）可用性；对时延敏感的搜索服务，用Elasticsearch分片+副本（近实时同步）平衡查询性能与数据一致。技术选择需匹配业务价值，而非堆砌工具——比如中小规模系统无需过度设计分片，优先优化单机索引和缓存，这是分布式设计的务实原则。
--------------------------------------------------
问题: 请讲述你在高并发应用开发方面的经验，包括遇到的高并发场景、采取的性能调优措施以及取得的效果
答案: 在之前负责电商平台大促订单系统时，曾面临日均百万订单、峰值QPS超1.5万的高并发场景，初期因数据库读写冲突导致接口响应延迟达800ms，偶发超时错误。

针对该场景，我们从三层优化：缓存层采用Redis集群部署，提前3天预热商品库存、用户购物车等热点数据，并用布隆过滤器过滤无效商品ID（拦截95%缓存穿透请求），同时对爆款商品加互斥锁防缓存击穿；数据库层实施读写分离（主库写入、8个从库读取），按用户ID哈希分8个订单库，订单表按时间范围分表，优化索引（新增用户+订单状态联合索引）；应用层通过RabbitMQ异步化订单状态通知（如物流、积分更新），削峰填谷处理下单请求，并配置Sentinel熔断非核心接口（如优惠券校验）。

优化后，系统峰值QPS提升至3万+，接口响应时间降至120ms内，大促期间零宕机，订单成功率从92%升至99.6%，数据库CPU负载从85%降至40%以下。
--------------------------------------------------